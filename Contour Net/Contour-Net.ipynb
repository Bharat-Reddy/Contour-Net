{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ToJsxRi4yxhJ",
    "outputId": "ccfab083-ae0f-4738-915f-78fedd88f6fd"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Mi77isE7zUJb",
    "outputId": "7ec92696-d334-487d-a312-dc54f854f87c"
   },
   "outputs": [],
   "source": [
    "!pip install SimpleITK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DR0yml9hzwrw"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['image.cmap'] = 'gray' \n",
    "from glob import glob\n",
    "import SimpleITK as sitk\n",
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 18\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "from ipywidgets import interact, interactive\n",
    "from ipywidgets import widgets\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch as t\n",
    "# t.backends.cudnn.benchmark = True\n",
    "t.backends.cudnn.enabled = True\n",
    "from torch.utils import data\n",
    "from torchvision import transforms as tsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jGb5tqIu0A7p",
    "outputId": "72345516-2d75-40bb-ae7d-77ba04b5204b"
   },
   "outputs": [],
   "source": [
    "!mkdir paths\n",
    "TRAIN_PATH = '/content/paths/trainpddca15_crp_v2_pool1.pth'\n",
    "TEST_PATH = '/content/paths/testpddca15_crp_v2_pool1.pth'\n",
    "CET_PATH = '/content/paths/trainpddca15_cet_crp_v2_pool1.pth'\n",
    "PET_PATH = '/content/paths/trainpddca15_pet_crp_v2_pool1.pth'\n",
    "#PET_PATH = '/content/paths/trainpddca15_pet_crp_v2_pool1.pth'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mIbblgUw0-sw"
   },
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import math\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "def getdatamaskfilenames(path, maskname):\n",
    "    data, masks_data = [], []\n",
    "    for pth in path: # get data files and mask files\n",
    "        maskfiles = []\n",
    "        for seg in maskname:\n",
    "            if os.path.exists(os.path.join(pth, './structures/'+seg+'_crp_v2.npy')):\n",
    "                maskfiles.append(os.path.join(pth, './structures/'+seg+'_crp_v2.npy'))\n",
    "            else:\n",
    "                print('missing annotation', seg, pth.split('/')[-1])\n",
    "                maskfiles.append(None)\n",
    "        data.append(os.path.join(pth, 'img_crp_v2.npy'))\n",
    "        masks_data.append(maskfiles)\n",
    "    return data, masks_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ubcnV8TK1MUN"
   },
   "outputs": [],
   "source": [
    "def imfit(img, newz, newy, newx):\n",
    "    z, y, x = img.shape\n",
    "    retimg = np.zeros((newz, newy, newx), img.dtype)\n",
    "    bz, ez = newz/2, newz/2+1\n",
    "    while ez - bz < z:\n",
    "        if bz - 1 >= 0:\n",
    "            bz -= 1\n",
    "        if ez - bz < z:\n",
    "            if ez + 1 <= z:\n",
    "                ez += 1\n",
    "    by, ey = newy/2, newy/2+1\n",
    "    while ey - by < y:\n",
    "        if by - 1 >= 0:\n",
    "            by -= 1\n",
    "        if ey - by < y:\n",
    "            if ey + 1 <= y:\n",
    "                ey += 1\n",
    "    bx, ex = newx/2, newx/2+1\n",
    "    while ex - bx < x:\n",
    "        if bx - 1 >= 0:\n",
    "            bx -= 1\n",
    "        if ex - bx < x:\n",
    "            if ex + 1 <= x:\n",
    "                ex += 1\n",
    "    bz = int(bz)\n",
    "    ez = int(ez)\n",
    "    by = int(by)\n",
    "    ey = int(ey)\n",
    "    bx = int(bx)\n",
    "    ex = int(ex)\n",
    "    retimg[bz:ez, by:ey, bx:ex] = img\n",
    "    return retimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0TiUkYAR1Vgm"
   },
   "outputs": [],
   "source": [
    "def getdatamask(data, mask_data, debug=False): # read data and mask, reshape\n",
    "    datas = []\n",
    "    for fnm, masks in tqdm(zip(data, mask_data)):\n",
    "        item = {}\n",
    "        img = np.load(fnm) # z y x\n",
    "        nz, ny, nx = img.shape\n",
    "        tnz, tny, tnx = math.ceil(nz/8.)*8., math.ceil(ny/8.)*8., math.ceil(nx/8.)*8.\n",
    "        img = imfit(img, int(tnz), int(tny), int(tnx)) #zoom(img, (tnz/nz,tny/ny,tnx/nx), order=2, mode='nearest')\n",
    "        item['img'] = t.from_numpy(img)\n",
    "        item['mask'] = []\n",
    "        for idx, maskfnm in enumerate(masks):\n",
    "            if maskfnm is None: \n",
    "                ms = np.zeros((nz, ny, nx), np.uint8)\n",
    "            else: \n",
    "                ms = np.load(maskfnm).astype(np.uint8)\n",
    "                assert ms.min() == 0 and ms.max() == 1\n",
    "            mask = imfit(ms, int(tnz), int(tny), int(tnx)) #zoom(ms, (tnz/nz,tny/ny,tnx/nx), order=0, mode='constant')\n",
    "            item['mask'].append(mask)\n",
    "        assert len(item['mask']) == 9\n",
    "        item['name'] = str(fnm)#.split('/')[-1]\n",
    "        datas.append(item)\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KqCFCXad1cBN"
   },
   "outputs": [],
   "source": [
    "def process(path='/content/gdrive/My Drive/pddca/pddca18/', debug=False):\n",
    "    trfnmlst, trfnmlstopt, tefnmlstoff, tefnmlst = [], [], [], [] # get train and test files\n",
    "    train_files, train_filesopt, test_filesoff, test_files = [], [], [], [] # MICCAI15 and MICCAI16 use different test\n",
    "    for pid in os.listdir(path):\n",
    "        if '0522c0001' <= pid <= '0522c0328':\n",
    "            trfnmlst.append(pid)\n",
    "            train_files.append(os.path.join(path, pid))\n",
    "        elif '0522c0329' <= pid <= '0522c0479':\n",
    "            trfnmlstopt.append(pid)\n",
    "            train_filesopt.append(os.path.join(path, pid))\n",
    "        elif '0522c0555' <= pid <= '0522c0746':\n",
    "            tefnmlstoff.append(pid)\n",
    "            test_filesoff.append(os.path.join(path, pid))\n",
    "        elif '0522c0788' <= pid <= '0522c0878':\n",
    "            tefnmlst.append(pid)\n",
    "            test_files.append(os.path.join(path, pid))\n",
    "        #else:\n",
    "         #   print(pid)\n",
    "          #  assert 1 == 0\n",
    "    print('train file names', trfnmlst)\n",
    "    print('optional train file names', trfnmlstopt)\n",
    "    print('offsite test file names', tefnmlstoff)\n",
    "    print('onsite test file names', tefnmlst)\n",
    "    print('Total train files', len(train_files), 'total test files', len(test_files))\n",
    "    print('Train optional files', len(train_filesopt), 'test optional files', len(test_filesoff))\n",
    "    assert len(trfnmlst) == 25 and len(trfnmlstopt) == 8 and len(tefnmlstoff) == 10 and len(tefnmlst) == 5\n",
    "    assert len(train_files) == 25 and len(train_filesopt) == 8 and len(test_filesoff) == 10 and     len(test_files) == 5\n",
    "    structurefnmlst = ('BrainStem', 'Spinal_Cord', 'Mandible', 'OpticNerve_L', 'OpticNerve_R', 'Parotid_L', 'Parotid_R', 'Submandibular_L', 'Submandibular_R')\n",
    "    train_data, train_masks_data = getdatamaskfilenames(train_files, structurefnmlst)\n",
    "    train_dataopt, train_masks_dataopt = getdatamaskfilenames(train_filesopt, structurefnmlst)\n",
    "    test_data, test_masks_data = getdatamaskfilenames(test_files, structurefnmlst)\n",
    "    test_dataoff, test_masks_dataoff = getdatamaskfilenames(test_filesoff, structurefnmlst)\n",
    "    return getdatamask(train_data+train_dataopt+test_data,                        train_masks_data+train_masks_dataopt+test_masks_data,debug=debug), getdatamask(test_dataoff, test_masks_dataoff,debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j6NesOYu2XyZ"
   },
   "outputs": [],
   "source": [
    "def processCET(path, debug=False):\n",
    "    trfnmlst = [] # get train and test files\n",
    "    train_files = [] # MICCAI15 and MICCAI16 use different test\n",
    "    for pid in os.listdir(path):\n",
    "        trfnmlst.append(pid)\n",
    "        train_files.append(os.path.join(path, pid))\n",
    "    print('train file names', trfnmlst)\n",
    "    print('Total train files', len(train_files))\n",
    "    structurefnmlst = ('BrainStem', 'Spinal_Cord', 'Mandible', 'OpticNerve_L', 'OpticNerve_R', 'Parotid_L', 'Parotid_R',                        'Submandibular_L', 'Submandibular_R')\n",
    "    train_data, train_masks_data = getdatamaskfilenames(train_files, structurefnmlst)\n",
    "    return getdatamask(train_data, train_masks_data,debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "d8ERGE423J0f",
    "outputId": "b7fa27be-ace8-4f42-8cec-e4aa05caf1da"
   },
   "outputs": [],
   "source": [
    "# You can skip this if you have alreadly done it.\n",
    "if not os.path.isfile(TEST_PATH):\n",
    "    test_data = processCET('/content/gdrive/My Drive/testing-10-patients')\n",
    "    #test_data = processCET('/content/gdrive/My Drive/single-patient')\n",
    "   # print('use train', len(train_data), 'use test', len(test_data))\n",
    "    #t.save(train_data, TRAIN_PATH)\n",
    "    t.save(test_data, TEST_PATH)\n",
    "if not os.path.isfile(CET_PATH):\n",
    "    train_data, test_data = process('/content/gdrive/My Drive/pddca/pddca18')\n",
    "    print('use train', len(train_data), 'use test', len(test_data))\n",
    "    data = processCET('/content/gdrive/My Drive/HNCetuximabclean/')\n",
    "    print('use ', len(data))\n",
    "    t.save(data+train_data, CET_PATH)\n",
    "if not os.path.isfile(PET_PATH):\n",
    "    train_data = processC('/content/gdrive/My Drive/pddca/pddca18')\n",
    "    print('use train', len(train_data), 'use test', len(test_data))\n",
    "    data = processCET('/content/gdrive/My Drive/HNCetuximabclean/')\n",
    "    print('use ', len(data))\n",
    "    petdata = processCET('/content/gdrive/My Drive/hospital-full-data-HN-117-patients')\n",
    "    #petdata = processCET('/content/gdrive/My Drive/testing-10-patients')\n",
    "    print('use ', len(petdata))\n",
    "    t.save(petdata, PET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pv_x8Omg3YCC"
   },
   "outputs": [],
   "source": [
    "class DatasetStg1():\n",
    "    def __init__(self,path, istranform=True, alpha=1000, sigma=30, alpha_affine=0.04, istest=False):\n",
    "        self.datas = t.load(path)\n",
    "        self.ist = istranform\n",
    "        self.alpha = alpha\n",
    "        self.sigma = sigma\n",
    "        self.alpha_affine = alpha_affine\n",
    "        self.istest = istest\n",
    "    def __getitem__(self, index):\n",
    "        data = self.datas[index]\n",
    "        img = data['img'].numpy().astype(np.float32)\n",
    "        if not self.istest:\n",
    "            for mask in data['mask']: # for multi-task \n",
    "                if mask is None: \n",
    "                    print(data['name'])\n",
    "                    assert 1 == 0\n",
    "        if not self.ist: #[::2, ::2, ::2]\n",
    "            masklst = []\n",
    "            for mask in data['mask']:\n",
    "                if mask is None: mask = np.zeros((1,img.shape[0],img.shape[1],img.shape[2])).astype(np.uint8)\n",
    "                masklst.append(mask.astype(np.uint8).reshape((1,img.shape[0],img.shape[1],img.shape[2]))) \n",
    "            mask0 = np.zeros_like(masklst[0]).astype(np.uint8)\n",
    "            for mask in masklst:\n",
    "                mask0 = np.logical_or(mask0, mask).astype(np.uint8)\n",
    "            mask0 = 1 - mask0\n",
    "            return t.from_numpy(img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))),                    t.from_numpy(np.concatenate([mask0]+masklst, axis=0)), True\n",
    "        im_merge = np.concatenate([img[...,None]]+[mask.astype(np.float32)[...,None] for mask in data['mask']],                                  axis=3)\n",
    "        # Apply transformation on image\n",
    "        im_merge_t, new_img = self.elastic_transform3Dv2(im_merge,self.alpha,self.sigma,min(im_merge.shape[1:-1])*self.alpha_affine)\n",
    "        # Split image and mask ::2, ::2, ::2\n",
    "        im_t = im_merge_t[...,0]\n",
    "        im_mask_t = im_merge_t[..., 1:].astype(np.uint8).transpose(3, 0, 1, 2)\n",
    "        mask0 = np.zeros_like(im_mask_t[0, :, :, :]).reshape((1,)+im_mask_t.shape[1:]).astype(np.uint8)\n",
    "        im_mask_t_lst = []\n",
    "        flagvect = np.ones((10,), np.float32)\n",
    "        retflag = True\n",
    "        for i in range(9):\n",
    "            im_mask_t_lst.append(im_mask_t[i,:,:,:].reshape((1,)+im_mask_t.shape[1:]))\n",
    "            if im_mask_t[i,:,:,:].max() != 1: \n",
    "                retflag = False\n",
    "                flagvect[i+1] = 0\n",
    "            mask0 = np.logical_or(mask0, im_mask_t[i,:,:,:]).astype(np.uint8)\n",
    "        if not retflag: flagvect[0] = 0\n",
    "        mask0 = 1 - mask0\n",
    "        return t.from_numpy(im_t.reshape((1,)+im_t.shape[:3])),                t.from_numpy(np.concatenate([mask0]+im_mask_t_lst, axis=0)), flagvect\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "    def elastic_transform3Dv2(self, image, alpha, sigma, alpha_affine, random_state=None):\n",
    "        \"\"\"Elastic deformation of images as described in [Simard2003]_ (with modifications).\n",
    "        .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
    "             Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
    "             Proc. of the International Conference on Document Analysis and\n",
    "             Recognition, 2003.\n",
    "         Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5\n",
    "         From https://www.kaggle.com/bguberfain/elastic-transform-for-data-augmentation\n",
    "        \"\"\"\n",
    "        # affine and deformation must be slice by slice and fixed for slices\n",
    "        if random_state is None:\n",
    "            random_state = np.random.RandomState(None)\n",
    "        shape = image.shape # image is contatenated, the first channel [:,:,:,0] is the image, the second channel \n",
    "        # [:,:,:,1] is the mask. The two channel are under the same tranformation.\n",
    "        shape_size = shape[:-1] # z y x\n",
    "        # Random affine\n",
    "        shape_size_aff = shape[1:-1] # y x\n",
    "        center_square = np.float32(shape_size_aff) // 2\n",
    "        square_size = min(shape_size_aff) // 3\n",
    "        pts1 = np.float32([center_square + square_size, [center_square[0]+square_size, center_square[1]-square_size],                           center_square - square_size])\n",
    "        pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)\n",
    "        M = cv2.getAffineTransform(pts1, pts2)\n",
    "        new_img = np.zeros_like(image)\n",
    "        for i in range(shape[0]):\n",
    "            new_img[i,:,:,0] = cv2.warpAffine(image[i,:,:,0], M, shape_size_aff[::-1],                                               borderMode=cv2.BORDER_CONSTANT, borderValue=0.)\n",
    "            for j in range(1, 10):\n",
    "                new_img[i,:,:,j] = cv2.warpAffine(image[i,:,:,j], M, shape_size_aff[::-1], flags=cv2.INTER_NEAREST,                                                  borderMode=cv2.BORDER_TRANSPARENT, borderValue=0)\n",
    "        dx = gaussian_filter((random_state.rand(*shape[1:-1]) * 2 - 1), sigma) * alpha\n",
    "        dy = gaussian_filter((random_state.rand(*shape[1:-1]) * 2 - 1), sigma) * alpha\n",
    "        x, y = np.meshgrid(np.arange(shape_size_aff[1]), np.arange(shape_size_aff[0]))\n",
    "        indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1))\n",
    "        new_img2 = np.zeros_like(image)\n",
    "        for i in range(shape[0]):\n",
    "            new_img2[i,:,:,0] = map_coordinates(new_img[i,:,:,0], indices, order=1, mode='constant').reshape(shape[1:-1])\n",
    "            for j in range(1, 10):\n",
    "                new_img2[i,:,:,j] = map_coordinates(new_img[i,:,:,j], indices, order=0, mode='constant').reshape(shape[1:-1])\n",
    "        return np.array(new_img2), new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rKWk4To_4k90",
    "outputId": "a0a7c63a-71a1-4abc-fd61-35e1336e18cc"
   },
   "outputs": [],
   "source": [
    "traindataset = DatasetStg1(PET_PATH, istranform=True)\n",
    "traindataloader = t.utils.data.DataLoader(traindataset,num_workers=0,batch_size=1, shuffle=True)\n",
    "testdataset = DatasetStg1(TEST_PATH, istranform=False)\n",
    "testdataloader = t.utils.data.DataLoader(testdataset,num_workers=0,batch_size=1)\n",
    "print(len(traindataloader), len(testdataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kqSgqUby4qEE"
   },
   "outputs": [],
   "source": [
    "# sub-parts of the U-Net model\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.distance import dice\n",
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3x3 convolution with padding\"\n",
    "    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "class BasicBlock3D(nn.Module):\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super(BasicBlock3D, self).__init__()\n",
    "        self.conv1 = conv3x3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        if inplanes != planes:\n",
    "            self.downsample = nn.Sequential(nn.Conv3d(inplanes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                                            nn.BatchNorm3d(planes))\n",
    "        else:\n",
    "            self.downsample = lambda x: x\n",
    "        self.stride = stride       \n",
    "    def forward(self, x):\n",
    "#         print(x.size())\n",
    "        residual = self.downsample(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "#         print(x.size(), residual.size(), out.size())\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "def Deconv3x3x3(in_planes, out_planes, stride=2):\n",
    "    \"3x3x3 deconvolution with padding\"\n",
    "    return nn.ConvTranspose3d(in_planes, out_planes, kernel_size=2, stride=stride)\n",
    "\n",
    "class SELayer3D(nn.Module):\n",
    "    def __init__(self, channel, reduction=15):\n",
    "        super(SELayer3D, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(channel, channel // reduction),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(channel // reduction, channel),\n",
    "                nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        b, c, _, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1, 1)\n",
    "        return x * y\n",
    "class SEBasicBlock3D(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=15):\n",
    "        super(SEBasicBlock3D, self).__init__()\n",
    "        self.conv1 = conv3x3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes, 1)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.se = SELayer3D(planes, reduction)\n",
    "        if inplanes != planes:\n",
    "            self.downsample = nn.Sequential(nn.Conv3d(inplanes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                                            nn.BatchNorm3d(planes))\n",
    "        else:\n",
    "            self.downsample = lambda x: x\n",
    "        self.stride = stride\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)\n",
    "#         if self.downsample is not None:\n",
    "        residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "class UpSEBasicBlock3D(nn.Module):\n",
    "    def __init__(self, inplanes1, inplanes2, planes, stride=1, downsample=None, reduction=16):\n",
    "        super(UpSEBasicBlock3D, self).__init__()\n",
    "        inplanes3 = inplanes1 + inplanes2\n",
    "        if stride == 2:\n",
    "            self.deconv1 = Deconv3x3x3(inplanes1, inplanes1//2)\n",
    "            inplanes3 = inplanes1 // 2 + inplanes2\n",
    "        self.stride = stride\n",
    "        # self.conv1x1x1 = nn.Conv3d(inplanes2, planes, kernel_size=1, stride=1)#, padding=1)\n",
    "        self.conv1 = conv3x3x3(inplanes3, planes)#, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.se = SELayer3D(planes, reduction)\n",
    "        if inplanes3 != planes:\n",
    "            self.downsample = nn.Sequential(nn.Conv3d(inplanes3, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                                            nn.BatchNorm3d(planes))\n",
    "        else:\n",
    "            self.downsample = lambda x: x\n",
    "        self.stride = stride\n",
    "    def forward(self, x1, x2):\n",
    "#         print(x1.size(), x2.size())\n",
    "        if self.stride == 2: x1 = self.deconv1(x1)\n",
    "        # x2 = self.conv1x1x1(x2)\n",
    "        #print(x1.size(), x2.size())\n",
    "        out = t.cat([x1, x2], dim=1) #x1 + x2\n",
    "        residual = self.downsample(out)\n",
    "        #print(residual.size(), x1.size(), x2.size())\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)\n",
    "        #print(out.size(), residual.size())\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "class UpBasicBlock3D(nn.Module):\n",
    "    def __init__(self, inplanes1, inplanes2, planes, stride=2):\n",
    "        super(UpBasicBlock3D, self).__init__()\n",
    "        inplanes3 = inplanes1 + inplanes2\n",
    "        if stride == 2:\n",
    "            self.deconv1 = Deconv3x3x3(inplanes1, inplanes1//2)\n",
    "            inplanes3 = inplanes1//2 + inplanes2\n",
    "        self.stride = stride\n",
    "        # elif inplanes1 != planes:\n",
    "            # self.deconv1 = nn.Conv3d(inplanes1, planes, kernel_size=1, stride=1)\n",
    "        # self.conv1x1x1 = nn.Conv3d(inplanes2, planes, kernel_size=1, stride=1)#, padding=1)\n",
    "        self.conv1 = conv3x3x3(inplanes3, planes)#, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        if inplanes3 != planes:\n",
    "            self.downsample = nn.Sequential(nn.Conv3d(inplanes3, planes, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                            nn.BatchNorm3d(planes))\n",
    "        else:\n",
    "            self.downsample = lambda x: x\n",
    "        self.stride = stride\n",
    "    def forward(self, x1, x2):\n",
    "#         print(x1.size(), x2.size())\n",
    "        if self.stride == 2: x1 = self.deconv1(x1)\n",
    "        #print(self.stride, x1.size(), x2.size())\n",
    "        out = t.cat([x1, x2], dim=1)\n",
    "        residual = self.downsample(out)\n",
    "        #print(out.size(), residual.size())\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "class ResNetUNET3D(nn.Module):\n",
    "    def __init__(self, block, upblock, upblock1, n_size, num_classes=2, in_channel=1): # BasicBlock, 3\n",
    "        super(ResNetUNET3D, self).__init__()\n",
    "        self.inplane = 28\n",
    "        self.conv1 = nn.Conv3d(in_channel, self.inplane, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(self.inplane)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 30, blocks=n_size, stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, blocks=n_size, stride=1)\n",
    "        self.layer3 = self._make_layer(block, 34, blocks=n_size, stride=1)\n",
    "        self.layer4 = upblock(34, 32, 32, stride=1)\n",
    "        self.inplane = 32\n",
    "        self.layer5 = self._make_layer(block, 32, blocks=n_size-1, stride=1)\n",
    "        self.layer6 = upblock(32, 30, 30, stride=1)\n",
    "        self.inplane = 30\n",
    "        self.layer7 = self._make_layer(block, 30, blocks=n_size-1, stride=1)\n",
    "        self.layer8 = upblock(30, 28, 28, stride=1)\n",
    "        self.inplane = 28\n",
    "        self.layer9 = self._make_layer(block, 28, blocks=n_size-1, stride=1)\n",
    "        self.inplane = 28\n",
    "        self.layer10 = upblock1(28, 1, 14, stride=2)\n",
    "        self.layer11 = nn.Sequential(#nn.Conv3d(16, 14, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                                     #nn.ReLU(inplace=True),\n",
    "                                     nn.Conv3d(14, num_classes, kernel_size=3, stride=1, padding=1, bias=True))\n",
    "#         self.outconv = nn.ConvTranspose3d(self.inplane, num_classes, 2, stride=2)\n",
    "        self.initialize()\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.ConvTranspose3d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "    def _make_layer(self, block, planes, blocks, stride):\n",
    "        strides = [stride] + [1] * (blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inplane, planes, stride))\n",
    "            self.inplane = planes\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x0):\n",
    "        x = self.conv1(x0) # 16 1/2 \n",
    "        x = self.bn1(x)\n",
    "        x1 = self.relu(x)\n",
    "\n",
    "        x2 = self.layer1(x1) # 16 1/4 16 1/4 res 16 1/4 - 16 1/4 16 1/4 res 16 1/4 - 16 1/4 16 1/4 res 16 1/4\n",
    "        x3 = self.layer2(x2) # 32 1/8 32 1/8 res 32 1/8 - 32 1/8 32 1/8 res 32 1/8 - 32 1/8 32 1/8 res 32 1/8\n",
    "        x4 = self.layer3(x3) # 64 1/16 64 1/16 res 64 1/16 - 64 1/16 64 1/16 res 64 1/16 - 64 1/16 64 1/16 res 64 1/16\n",
    "#         print('x4', x4.size())\n",
    "        x5 = self.layer4(x4, x3) # 16 1/8 48 1/8 32 1/8 32 1/8 res 32 1/8 - 32 1/8 32 1/8 res 32 1/8 - 32 1/8 32 1/8 res 32 1/8\n",
    "        x5 = self.layer5(x5)\n",
    "        x6 = self.layer6(x5, x2) # 8 1/4 24 1/4 16 1/4 16 1/4 res 16 1/4 - 16 1/4 16 1/4 res 16 1/4 - 16 1/4 16 1/4 res 16 1/4\n",
    "        x6 = self.layer7(x6)\n",
    "        x7 = self.layer8(x6, x1) # 4 1/2 20 1/2 16 1/2 16 1/2 res 16 1/2 - 16 1/2 16 1/2 res 16 1/2 - 16 1/2 16 1/2 res 16 1/2\n",
    "        x7 = self.layer9(x7)\n",
    "        x8 = self.layer10(x7, x0)\n",
    "        x9 = self.layer11(x8)\n",
    "#         print(x0.size(), x.size(), x1.size(), x2.size(), x3.size(), x4.size(), x5.size(), x6.size(), \\\n",
    "#               x7.size(), x8.size(), x9.size())\n",
    "        return F.softmax(x9, dim=1)\n",
    "#         out = self.outconv(x7)\n",
    "#         return F.softmax(out, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ENJOji-NtEqA",
    "outputId": "a4edc60b-8096-4f57-c96f-4fa2cffecc2c"
   },
   "outputs": [],
   "source": [
    "def crossentropy(y_pred, y_true, flagvec):\n",
    "    retv = - t.sum(t.sum(t.sum(t.sum(t.log(t.clamp(y_pred,1e-6,1))*y_true.type(t.cuda.FloatTensor),4),3),2),0) * flagvec.cuda() \n",
    "    return t.sum(retv / (t.sum(t.sum(t.sum(t.sum(y_true.type(t.cuda.FloatTensor),4),3),2),0) + 1e-6)) / y_true.size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JaaU_CsYQuHz"
   },
   "outputs": [],
   "source": [
    "def focal(y_pred, y_true, flagvec):\n",
    "    retv = - t.mean(t.mean(t.mean(t.mean(t.log(t.clamp(y_pred,1e-6,1))*y_true.type(t.cuda.FloatTensor)*t.pow(1-y_pred,2),4),3),2),0)\\\n",
    "        * flagvec.cuda()\n",
    "    return retv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84VsS1uc5_P7"
   },
   "outputs": [],
   "source": [
    "# Ref: salehi17, \"Twersky loss function for image segmentation using 3D FCDN\"\n",
    "# -> the score is computed for each class separately and then summed\n",
    "# alpha=beta=0.5 : dice coefficient\n",
    "# alpha=beta=1   : tanimoto coefficient (also known as jaccard)\n",
    "# alpha+beta=1   : produces set of F*-scores\n",
    "# implemented by E. Moebel, 06/04/18\n",
    "def tversky_loss_wmask(y_pred, y_true, flagvec):\n",
    "    alpha = 0.5\n",
    "    beta  = 0.5\n",
    "    ones = t.ones_like(y_pred) #K.ones(K.shape(y_true))\n",
    "#     print(type(ones.data), type(y_true.data), type(y_pred.data), ones.size(), y_pred.size())\n",
    "    p0 = y_pred      # proba that voxels are class i\n",
    "    p1 = ones-y_pred # proba that voxels are not class i\n",
    "    g0 = y_true.type(t.cuda.FloatTensor)\n",
    "    g1 = ones-g0\n",
    "    num = t.sum(t.sum(t.sum(t.sum(p0*g0, 4),3),2),0) #(0,2,3,4)) #K.sum(p0*g0, (0,1,2,3))\n",
    "    den = num + alpha*t.sum(t.sum(t.sum(t.sum(p0*g1,4),3),2),0) + beta*t.sum(t.sum(t.sum(t.sum(p1*g0,4),3),2),0) #(0,2,3,4))\n",
    "\n",
    "    T = t.sum((num * flagvec.cuda())/(den+1e-5))\n",
    "\n",
    "#     Ncl = y_pred.size(1)*1.0\n",
    "#     print(Ncl, T)\n",
    "    return t.sum(flagvec.cuda())-T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hl2n4qkG6EId"
   },
   "outputs": [],
   "source": [
    "def caldice(y_pred, y_true):\n",
    "#     print(y_pred.sum(), y_true.sum())\n",
    "    y_pred = y_pred.data.cpu().numpy().transpose(1,0,2,3,4) # inference should be arg max\n",
    "    y_pred = np.argmax(y_pred, axis=0).squeeze() # z y x\n",
    "    y_true = y_true.data.numpy().transpose(1,0,2,3,4).squeeze() # .cpu()\n",
    "    avgdice = []\n",
    "    y_pred_1 = y_pred==1\n",
    "    y_true_1 = y_true[1,:,:,:]\n",
    "    if y_pred_1.sum() + y_true_1.sum() == 0: avgdice.append(-1)\n",
    "    else: avgdice.append(2.*(np.logical_and(y_pred_1, y_true_1).sum()) / (1.0*(y_pred_1.sum() + y_true_1.sum())))\n",
    "    \n",
    "    y_pred_1 = y_pred==2\n",
    "    y_true_1 = y_true[2,:,:,:]\n",
    "    if y_pred_1.sum() + y_true_1.sum() == 0: avgdice.append(-1)\n",
    "    else: avgdice.append(2.*(np.logical_and(y_pred_1, y_true_1).sum()) / (1.0*(y_pred_1.sum() + y_true_1.sum())))\n",
    "    \n",
    "    y_pred_1 = y_pred==3\n",
    "    y_true_1 = y_true[3,:,:,:]\n",
    "    if y_pred_1.sum() + y_true_1.sum() == 0: avgdice.append(-1)\n",
    "    else: avgdice.append(2.*(np.logical_and(y_pred_1, y_true_1).sum()) / (1.0*(y_pred_1.sum() + y_true_1.sum())))\n",
    "    \n",
    "    y_pred_1 = y_pred==4\n",
    "    y_true_1 = y_true[4,:,:,:]\n",
    "    if y_pred_1.sum() + y_true_1.sum() == 0: avgdice.append(-1)\n",
    "    else: avgdice.append(2.*(np.logical_and(y_pred_1, y_true_1).sum()) / (1.0*(y_pred_1.sum() + y_true_1.sum())))\n",
    "    \n",
    "    y_pred_1 = y_pred==5\n",
    "    y_true_1 = y_true[5,:,:,:]\n",
    "    if y_pred_1.sum() + y_true_1.sum() == 0: avgdice.append(-1)\n",
    "    else: avgdice.append(2.*(np.logical_and(y_pred_1, y_true_1).sum()) / (1.0*(y_pred_1.sum() + y_true_1.sum())))\n",
    "    \n",
    "    y_pred_1 = y_pred==6\n",
    "    y_true_1 = y_true[6,:,:,:]\n",
    "    if y_pred_1.sum() + y_true_1.sum() == 0: avgdice.append(-1)\n",
    "    else: avgdice.append(2.*(np.logical_and(y_pred_1, y_true_1).sum()) / (1.0*(y_pred_1.sum() + y_true_1.sum())))\n",
    "    \n",
    "    y_pred_1 = y_pred==7\n",
    "    y_true_1 = y_true[7,:,:,:]\n",
    "    if y_pred_1.sum() + y_true_1.sum() == 0: avgdice.append(-1)\n",
    "    else: avgdice.append(2.*(np.logical_and(y_pred_1, y_true_1).sum()) / (1.0*(y_pred_1.sum() + y_true_1.sum())))\n",
    "    \n",
    "    y_pred_1 = y_pred==8\n",
    "    y_true_1 = y_true[8,:,:,:]\n",
    "    if y_pred_1.sum() + y_true_1.sum() == 0: avgdice.append(-1)\n",
    "    else: avgdice.append(2.*(np.logical_and(y_pred_1, y_true_1).sum()) / (1.0*(y_pred_1.sum() + y_true_1.sum())))\n",
    "    \n",
    "    y_pred_1 = y_pred==9\n",
    "    y_true_1 = y_true[9,:,:,:]\n",
    "    if y_pred_1.sum() + y_true_1.sum() == 0: avgdice.append(-1)\n",
    "    else: avgdice.append(2.*(np.logical_and(y_pred_1, y_true_1).sum()) / (1.0*(y_pred_1.sum() + y_true_1.sum())))\n",
    "    for dice in avgdice: \n",
    "        if dice != -1:\n",
    "            assert 0 <= dice <= 1\n",
    "    return avgdice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zQ4Oz9GuLQCm",
    "outputId": "7e49d7b6-52b2-456f-89ae-1f9106d8bc8e"
   },
   "outputs": [],
   "source": [
    "#Adding this extra duplicate code from above for tversky lodd (alpha=0.3 && beta=0.7)\n",
    "!mkdir '/content/gdrive/My Drive/tversky-focal-test'\n",
    "model = ResNetUNET3D(SEBasicBlock3D, UpSEBasicBlock3D, UpBasicBlock3D, 2, num_classes=9+1, in_channel=1).cuda() \n",
    "lossweight = np.array([2.22, 1.06, 1.02, 1.74, 1.93, 1.93, 1.13, 1.15, 1.90, 1.98], np.float32)\n",
    "ceweight = 0.05\n",
    "focweight = 0.5\n",
    "savename = '/content/gdrive/My Drive/tversky-focal-test/unet10pool3e2e_seres18_conc_pet_wmask_2_rmsp_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WQ-jhicp-MCy",
    "outputId": "9b5e81d9-d9ca-46a3-e9dd-a1324f0334c8"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(t.load('/content/gdrive/My Drive/tversky-focal/unet10pool3e2e_seres18_conc_pet_wmask_2_rmsp_3')[\"weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UhPOjNXoubO_"
   },
   "outputs": [],
   "source": [
    "optimizer = t.optim.RMSprop(model.parameters(),lr = 2e-3)\n",
    "maxloss = [0 for _ in range(9)]\n",
    "#maxloss = [0.8241, 0.4880, 0.5999, 0.4392, 0.6681, 0.8587, 0.8396, 0.7598, 0.7626]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KJK3wxbb7Aby",
    "outputId": "000f0fab-3eb7-4e23-d961-c21dc4ba9bd6"
   },
   "outputs": [],
   "source": [
    "# In[5]:\n",
    "for epoch in range(150):\n",
    "    tq = tqdm(traindataloader, desc='loss', leave=True)\n",
    "    trainloss = 0\n",
    "    for x_train, y_train, flagvec in tq:\n",
    "        #########################\n",
    "        tq.set_description(\"epoch %i %s\" % (epoch, x_train.shape))\n",
    "        tq.refresh() # to show immediately the update\n",
    "        if x_train.shape[2] > 150:\n",
    "            x_train = x_train[:,:,37:37+64,...]\n",
    "            y_train = y_train[:,:,37:37+64,...]\n",
    "        if x_train.shape[-1] > 367:\n",
    "            x_train = x_train[...,50:200,50:200]\n",
    "            y_train = y_train[...,50:200,50:200]\n",
    "        #########################\n",
    "        x_train = t.autograd.Variable(x_train.cuda())\n",
    "        y_train = t.autograd.Variable(y_train.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        o = model(x_train)\n",
    "        loss = tversky_loss_wmask(o, y_train, flagvec*t.from_numpy(lossweight))\n",
    "        celoss = crossentropy(o, y_train, flagvec*t.from_numpy(lossweight))\n",
    "        floss = focal(o, y_train, flagvec*t.from_numpy(lossweight))\n",
    "        (loss + ceweight*celoss + focweight*floss).backward()\n",
    "        optimizer.step()\n",
    "        tq.set_description(\"epoch %i loss %f\" % (epoch, loss.item()))\n",
    "        tq.refresh() # to show immediately the update\n",
    "        trainloss += loss.item()\n",
    "        del loss, x_train, y_train, o\n",
    "    testtq = tqdm(testdataloader, desc='test loss', leave=True)\n",
    "    testloss = [0 for _ in range(9)]\n",
    "    for x_test, y_test, _ in testtq:\n",
    "#         print(x_test.numpy().shape)\n",
    "        with t.no_grad():\n",
    "            x_test = t.autograd.Variable(x_test.cuda())\n",
    "#             y_test = t.autograd.Variable(y_test.cuda())\n",
    "        o = model(x_test)\n",
    "        loss = caldice(o, y_test)\n",
    "        testtq.set_description(\"epoch %i test loss %f\" % (epoch, sum(loss)/9))\n",
    "        testtq.refresh() # to show immediately the update\n",
    "        testloss = [l+tl for l,tl in zip(loss, testloss)]\n",
    "        del x_test, y_test, o\n",
    "    testloss = [l / len(testtq) for l in testloss]\n",
    "    for cls in range(9):\n",
    "        if maxloss[cls] < testloss[cls]:\n",
    "            maxloss[cls] = testloss[cls]\n",
    "            state = {\"epoch\": epoch, \"weight\": model.state_dict()}\n",
    "            t.save(state, savename+str(cls+1))\n",
    "#             model.load_state_dict(t.load(savename)[\"weight\"])\n",
    "#             t.save(model, savename+str(cls+1))\n",
    "    print('epoch %i TRAIN loss %.4f' % (epoch, trainloss/len(tq)))\n",
    "    print('test loss %.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f' % tuple(testloss))\n",
    "    print('best test loss %.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f' % tuple(maxloss))\n",
    "    if epoch % 10 == 0:\n",
    "        testloss = [0 for _ in range(9)]\n",
    "        ntest = [0 for _ in range(9)]\n",
    "        testtq = tqdm(traindataloader, desc='loss', leave=True)\n",
    "        for x_test, y_test, _ in testtq:\n",
    "            if x_test.shape[2] > 150:\n",
    "                x_test = x_test[:,:,37:37+64,...]\n",
    "                y_test = y_test[:,:,37:37+64,...]\n",
    "            if x_test.shape[-1] > 367:\n",
    "                x_test = x_test[...,50:200,50:200]\n",
    "                y_test = y_test[...,50:200,50:200]  \n",
    "    #         print(x_test.numpy().shape)\n",
    "            with t.no_grad():\n",
    "                x_test = t.autograd.Variable(x_test.cuda())\n",
    "#                 y_test = t.autograd.Variable(y_test.cuda())\n",
    "            o = model(x_test)\n",
    "            loss = caldice(o, y_test)\n",
    "            testtq.set_description(\"epoch %i test loss %f\" % (epoch, sum(loss)/9))\n",
    "            testtq.refresh() # to show immediately the update\n",
    "            testloss = [l+tl if l != -1 else tl for l,tl in zip(loss, testloss)]\n",
    "            ntest = [n+1 if l != -1 else n for l, n in zip(loss, ntest)]\n",
    "            del x_test, y_test, o\n",
    "        testloss = [l / n for l,n in zip(testloss, ntest)]\n",
    "        print('train loss %.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f' % tuple(testloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jO1_Sq1W8NNy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3EX-YjtpkoRh",
    "outputId": "70efd9b0-09f3-4398-cab0-6aca254d23a6"
   },
   "outputs": [],
   "source": [
    "optimizer = t.optim.SGD(model.parameters(), 1e-3, momentum = 0.9)#, weight_decay = 1e-4)\n",
    "for epoch in range(50):\n",
    "    tq = tqdm(traindataloader, desc='loss', leave=True)\n",
    "    trainloss = 0\n",
    "    for x_train, y_train, flagvec in tq:\n",
    "        if x_train.shape[2] > 150:\n",
    "            x_train = x_train[:,:,37:37+64,...]\n",
    "            y_train = y_train[:,:,37:37+64,...]\n",
    "        if x_train.shape[-1] > 367:\n",
    "            x_train = x_train[...,50:200,50:200]\n",
    "            y_train = y_train[...,50:200,50:200]\n",
    "        x_train = t.autograd.Variable(x_train.cuda())\n",
    "        y_train = t.autograd.Variable(y_train.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        o = model(x_train)\n",
    "        loss = tversky_loss_wmask(o, y_train, flagvec*t.from_numpy(lossweight))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tq.set_description(\"epoch %i loss %f\" % (epoch, loss.item()))\n",
    "        tq.refresh() # to show immediately the update\n",
    "        trainloss += loss.item()\n",
    "        del loss, x_train, y_train, o\n",
    "    testtq = tqdm(testdataloader, desc='test loss', leave=True)\n",
    "    testloss = [0 for _ in range(9)]\n",
    "    for x_test, y_test, _ in testtq:\n",
    "#         print(x_test.numpy().shape)\n",
    "        with t.no_grad():\n",
    "            x_test = t.autograd.Variable(x_test.cuda())\n",
    "#             y_test = t.autograd.Variable(y_test.cuda())\n",
    "        o = model(x_test)\n",
    "        loss = caldice(o, y_test)\n",
    "        testtq.set_description(\"epoch %i test loss %f\" % (epoch, sum(loss)/9))\n",
    "        testtq.refresh() # to show immediately the update\n",
    "        testloss = [l+tl for l,tl in zip(loss, testloss)]\n",
    "        del x_test, y_test, o\n",
    "    testloss = [l / len(testtq) for l in testloss]\n",
    "    for cls in range(9):\n",
    "        if maxloss[cls] < testloss[cls]:\n",
    "            maxloss[cls] = testloss[cls]\n",
    "            state = {\"epoch\": epoch, \"weight\": model.state_dict()}\n",
    "            t.save(state, savename+str(cls+1))\n",
    "#             model.load_state_dict(t.load(savename)[\"weight\"])\n",
    "#             t.save(model, savename+str(cls+1))\n",
    "    print('epoch %i TRAIN loss %.4f' % (epoch, trainloss/len(tq)))\n",
    "    print('test loss %.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f' % tuple(testloss))\n",
    "    print('best test loss %.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f' % tuple(maxloss))\n",
    "    if epoch % 10 == 0:\n",
    "        testloss = [0 for _ in range(9)]\n",
    "        ntest = [0 for _ in range(9)]\n",
    "        testtq = tqdm(traindataloader, desc='loss', leave=True)\n",
    "        for x_test, y_test, _ in testtq:\n",
    "            if x_test.shape[2] > 150:\n",
    "                x_test = x_test[:,:,37:37+64,...]\n",
    "                y_test = y_test[:,:,37:37+64,...]\n",
    "            if x_test.shape[-1] > 367:\n",
    "                x_test = x_test[...,50:200,50:200]\n",
    "                y_test = y_test[...,50:200,50:200]\n",
    "    #         print(x_test.numpy().shape)\n",
    "            with t.no_grad():\n",
    "                x_test = t.autograd.Variable(x_test.cuda())\n",
    "#                 y_test = t.autograd.Variable(y_test.cuda())\n",
    "            o = model(x_test)\n",
    "            loss = caldice(o, y_test)\n",
    "            testtq.set_description(\"epoch %i test loss %f\" % (epoch, sum(loss)/9))\n",
    "            testtq.refresh() # to show immediately the update\n",
    "            testloss = [l+tl if l != -1 else tl for l,tl in zip(loss, testloss)]\n",
    "            ntest = [n+1 if l != -1 else n for l, n in zip(loss, ntest)]\n",
    "            del x_test, y_test, o\n",
    "        testloss = [l / n for l,n in zip(testloss, ntest)]\n",
    "        print('train loss %.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f' % tuple(testloss))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "tversky-focal-loss-new.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
